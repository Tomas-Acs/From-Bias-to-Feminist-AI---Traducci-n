# Capítulo 1: Nosotros formamos nuestras herramientas de la misma manera que nuestras herramientas nos forman

Esta máxima representa un peligro particular para las mujeres y niñas del sur global, históricamente marginalizadas y sistemáticamente excluidas de tecnología, toma de decisiones o de múltiples oportunidades para participar en crear sus propias soluciones.

Por: [Caitlin Kraft-Buchman](https://feministai.pubpub.org/user/caitlin-kraft-buchman)  

-----------------


 Como Marshall McLuhan dijo famosamente, “Nosotros formamos nuestras herramientas de la misma manera que nuestras herramientas nos forman”.  
 
Esta máxima representa un peligro particular para las mujeres y niñas del sur global, históricamente marginalizadas y sistemáticamente excluidas de tecnología, toma de decisiones o de múltiples oportunidades para participar en crear sus propias soluciones para los grandes problemas sociales que buscamos que la tecnología arregle. Esto es algo particularmente urgente dado la escala en la que la Inteligencia Artificial (AI) y los algoritmos de decisión (ADM) están siendo distribuidos globalmente como la normativa, están tan unidos al sistema que se vuelven maquinas inconscientes e intratables, las cuales ejecutan viejas normas y estereotipos que aprenden de otros patrones aprendidos de maquinas viejas.  


El aprendizaje automático (Machine learning) hace que la información que esta implícita en la información se materialice (incluida la “información faltante” de mujeres, niñas y otros grupos históricamente marginalizados haciéndolos invisibles en la información). Esta invisibilidad se vuelve explicita en el código ya que el aprendizaje automático “imita inteligentemente” la información que se la suministrado del mundo analógico.  De esta manera si las reglas de “genero” las cuales se van quitando paulatinamente del mundo, se programan dentro del AI y los algoritmos de decisión, estás van a volver de una manera mucho más fuerte y efectiva, creando un patriarcado 2.0 aún más difícil de erradicar que las actuales estructuras coloniales y patriarcales.  


Sin embargo, tenemos una pequeña oportunidad para crear nuevas normas ahora.  

Hay que actuar rápidamente. Compuesto por la pandemia del COVID-19, desarrollo acelerado publico y privado de despliegue de AI en el norte global, esto se va a ver reflejado en el norte como en el sur, aumentando el peligro de que el sur global va a ser abandonado y dejado atrás en investigación, aplicación y despliegue de AI; todavía menos AI enfocado en problemas del sur global o problemas del sur global feministas. La falta de presupuesto de investigación, capacidad, migración de talento y conflicto de prioridades pueden dejar al sur global sin opción más que adoptar tecnología que no está moldeada a sus realidad y necesidades. Esto va a ser una perdida para un mundo que necesita más invención y más diversidad en su innovación. En esta situación, se va a ampliar aún más la información obsoleta dentro de los modelos de datos, ampliando la exclusión para aquellos grupos históricamente marginalizados de mujeres y niñas. [^1]  


Es importante hacer notar que las mujeres y niñas pueden y son una forma de darle voz a todos aquellos grupos tradicionalmente invisibles, aquellos tradicionalmente dejados atrás. La información de datos feminista es inseparable de la “interseccionalidad”, “la naturaleza interconectada de categorías sociales como la clase, raza y genero mientras se aplican a cierto individuo o grupo, considerado como una superposición y sistemas interdependientes de discriminación y desventaja”. Nuestra discusión es igualmente poderosa (con ese propósito) y aplicable a otras formas de discriminación, más notablemente discriminación racial. [^2]  

Estamos en un punto de inflección.  

# El Paisaje

## Parcialidad de género en los algoritmos y decisiones automáticas  


### Parcialidad inherente al contratar

Para optimizar los recursos humanos Amazon creo un algoritmo derivado de 10 años de currículos enviados a Amazon. Usó información comparada con el equipo de alto rendimiento, principalmente de hombres, de ingeniería. Este algoritmo se le enseño como reconocer patrones de palabras, en lugar de habilidades importantes en los currículos, y al ver que históricamente los hombres eran contratados y promovidos, el algoritmo se enseño a si mismo a penalizar cualquier resumen que incluyera la palabra “mujeres”, por ejemplo “capitana del equipo de ajedrez de mujeres” en el texto y degradar resúmenes de mujeres que hubieran ido a “universidades de mujeres” [^3]. Esto se debe a que la información con la que se entrenó el algoritmo contiene parcialidad o discriminación histórica, crea un bucle donde el aprendizaje automático absorbe esta parcialidad y lo replica, incorporándolo a decisiones futuras, y hace que esta parcialidad implícita se vuelva una realidad explicita. [^4]  
 
 A pesar de múltiples intentos fracasados de arreglar el algoritmo y quitarle la parcialidad, la cual se asumió que iba a ser un simple arreglo técnico, Amazon eventualmente desecho el algoritmo en su totalidad ya que la parcialidad estaba demasiado arraigada en los procesos de reclutamiento anteriores. Esta parcialidad estaba profundamente implícita en la información en la cual el algoritmo se entrenó y el aprendizaje automático del sistema ADM no pudo “desaprenderlo” [^5]. En el 2017, Amazon abandonó el proyecto y compartió su información y experiencia con el público por Reuters.   
 
 La Inteligencia Artificial permea los procesos modernos de reclutamiento desde web crawlers para identificar y atraer a candidatos favoritos a través de sistemas de rastreo de aplicantes, curadores de currículos, evaluaciones clásicas, entrevistas automatizada, análisis de entrevistas y sistemas de calificación de candidatos. Un reporte reciente estima que el 99% de las compañías del Fortune 500 utilizan sistemas de rastreo de aplicantes de algún tipo en sus procesos de reclutamiento [^6]. La AI está esperada que reemplace el 16% de los trabajos de recursos en los próximos diez años [^7], lo cual significa que la dependencia multinacional de este tipo de software y procesos y accesos a la información (o la falta de) va a estar en crecimiento. 
 
 
 ### Parcialidad al seleccionar y los estereotipos
 
 Un estudio del 2019 de la funcionalidad de anuncios de Facebook encontró que los anuncios para los trabajos en la industria maderera eran mostrados desproporcionalmente a usuarios blancos masculinos, mientras que anuncios para posiciones de cajeros en supermercados eran enseñados a usuarios femeninos [^8]. Permitiendo que los anunciantes de trabajos solo llegaran a hombres, sin estar necesariamente de acuerdo o consientes de esto, se mostraron anuncios de una manera que se alineaba con los estereotipos de genero [^9]. Cuando un algoritmo “aprende” un patrón de que más hombres que mujeres están interesados en la industria maderera (incluso si no sabe su género y aprende esto correlacionando otra información sobre sus gustos y hábitos), entonces el sistema decide no mostrarles esos trabajos a otras mujeres, simplemente porque son mujeres [^10]. Esto exacerba los estereotipos existentes y las barreras en las sociedades que han excluido a las mujeres mucho antes de los sistemas automatizados.   
 
 Después de que Facebook fuera demandado por estas prácticas, como parte del arreglo a inicios del 2019, Facebook arregló cinco demandas y accedió detener a los anunciantes en ciertas categorías claves de poder llegar a grupos exclusivos de cierto tipo de personas, género o raza [^11].  
 
 Sin embargo, los algoritmos funcionan diferente en la teoría y en la práctica. Aun cuando Facebook quitó la funcionalidad de poder acceder a grupos específicos de personas, su complejo algoritmo en el 2021 depende de otras múltiples características que al final sirven como puente para llegar al género, raza y edad. Al final los resultados siempre terminan siendo parciales [^12].   
 
 Muchas veces los anuncios parecen deliberadamente tener estereotipos. Una encuesta de ProPublica encontró que la policía del estado de Pennsylvania en los Estados Unidos, por ejemplo, puso un anunció exclusivamente a hombres con el texto que decía: “La policía del estado de Pennsylvania inician con un salario de $59,567 al año. Aplique ahora.” [^13]. Los objetivos por genero es solo una manera en la cual las plataformas permiten a los anunciantes enfocarse en ciertos usuarios y excluir a otros, y por inadvertencia, denegarles la oportunidad de aplicar a trabajos con más paga y de más estatus.   
 
 ### Estereotipos implícitos y parcialidad inconsciente transducida a misoginia explicita  
 
 Investigadores de una compañía pionera de tecnología en Estados Unidos dicen tener un 97% de eficacia en un sistema de reconocimiento facial que diseñaron, sin embargo, la información con la que se construyó el sistema era mas de 77% hombres y mas de un 83% blancos [^14]. Investigadores del MIT y de Stanford en los Estados Unidos probaron tres sistemas de reconocimiento facial por IBM, Microsoft, Megvii, y encontrar que estos sistemas eran buenos para reconocer hombres blancos, pero no mujeres, específicamente si tenían un tono de piel mas oscuro [^15].   
 
 En el 2017 un grupo de investigadores encontró que dos prominentes colecciones de imágenes dedicadas para la investigación, incluyendo una soportada por Microsoft y Facebook, desplegaban una parcialidad de genero predecible en su representación de actividades como cocinar y deportes. Por ejemplo, imágenes de compras y lavado de platos estaban vinculadas con mujeres, mientras que imágenes de entramiento y disparos están vinculadas a hombres. De la misma manera, objetos de cocina como cucharas y tenedores están asociados fuertemente con mujeres, mientras que equipo de actividades al aire libre como tablas para hacer snowboarding y raquetas de tenis están asociadas a hombres. Otro lado de está información es la escasez geográfica de la información, en un estudio se muestra que los algoritmos muestran a la novia en una boda típica estadounidense de blanco como “novia”, “vestido”, “mujer”, mientras que una foto de una novia del norte de la india está clasificada como “arte”, “disfraz”. La parcialidad de la información usada para entrenar estos algoritmos sobre representa una población, mientras que minimiza otra [^16].    
 
 En el 2019 ImageNet quitó 600,000 imágenes de su sistema (entrenado con WordNet, la base de datos de palabras en Ingles usadas en lingüística computacional y procesamiento del lenguaje natural) después de que un proyecto de arte mostrado ImageNet Roulette ilustrara una parcialidad sistemática en su base de datos. En el 2020, el MIT quitó permanentemente 80 millones de imágenes pequeñas de bases de datos cuando descubrieron que estaban etiquetadas con contenido racista y misógino, produciendo resultados que inadvertidamente reafirmaban estereotipos y sesgos dañinos [^17].   
 
 El aprendizaje automático entrenado en estas bases de datos no solo replico estas parcialidades, las amplifico. Si un set de fotos está generalmente asociado con mujeres cocinando, este tipo de software entrenado estudiando esas fotos, crea asociaciones aun más fuertes. En las pruebas de los investigadores, personas que estaban en cocinas estaban más propensas a ser etiquetadas como “mujeres”: en una foto de un hombre en una cocina se le etiqueta como “mujer” [^18]. De la misma manera, investigadores de la Universidad de Washington y Maryland encontraron que algunos términos de búsqueda como “Chief Executive Officer” (CEO), Google presentaba porcentaje peores que las estadísticas actuales de la vida real. El estudio concluyó que 11% de las personas mostradas en la búsqueda de imágenes de CEO eran mujeres, mientras que la información muestra que, al momento de realizar la investigación, 27% de las mujeres en Estados Unidos eran CEOs. [^19]   
 
 
