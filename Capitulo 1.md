# Capítulo 1: Nosotros formamos nuestras herramientas de la misma manera que nuestras herramientas nos forman

Esta máxima representa un peligro particular para las mujeres y niñas del sur global, históricamente marginalizadas y sistemáticamente excluidas de tecnología, toma de decisiones o de múltiples oportunidades para participar en crear sus propias soluciones.

Por: [Caitlin Kraft-Buchman](https://feministai.pubpub.org/user/caitlin-kraft-buchman)  

-----------------


 Como Marshall McLuhan dijo famosamente, “Nosotros formamos nuestras herramientas de la misma manera que nuestras herramientas nos forman”.  
 
Esta máxima representa un peligro particular para las mujeres y niñas del sur global, históricamente marginalizadas y sistemáticamente excluidas de tecnología, toma de decisiones o de múltiples oportunidades para participar en crear sus propias soluciones para los grandes problemas sociales que buscamos que la tecnología arregle. Esto es algo particularmente urgente dado la escala en la que la Inteligencia Artificial (AI) y los algoritmos de decisión (ADM) están siendo distribuidos globalmente como la normativa, están tan unidos al sistema que se vuelven maquinas inconscientes e intratables, las cuales ejecutan viejas normas y estereotipos que aprenden de otros patrones aprendidos de maquinas viejas.  


El aprendizaje automático (Machine learning) hace que la información que esta implícita en la información se materialice (incluida la “información faltante” de mujeres, niñas y otros grupos históricamente marginalizados haciéndolos invisibles en la información). Esta invisibilidad se vuelve explicita en el código ya que el aprendizaje automático “imita inteligentemente” la información que se la suministrado del mundo analógico.  De esta manera si las reglas de “genero” las cuales se van quitando paulatinamente del mundo, se programan dentro del AI y los algoritmos de decisión, estás van a volver de una manera mucho más fuerte y efectiva, creando un patriarcado 2.0 aún más difícil de erradicar que las actuales estructuras coloniales y patriarcales.  


Sin embargo, tenemos una pequeña oportunidad para crear nuevas normas ahora.  

Hay que actuar rápidamente. Compuesto por la pandemia del COVID-19, desarrollo acelerado publico y privado de despliegue de AI en el norte global, esto se va a ver reflejado en el norte como en el sur, aumentando el peligro de que el sur global va a ser abandonado y dejado atrás en investigación, aplicación y despliegue de AI; todavía menos AI enfocado en problemas del sur global o problemas del sur global feministas. La falta de presupuesto de investigación, capacidad, migración de talento y conflicto de prioridades pueden dejar al sur global sin opción más que adoptar tecnología que no está moldeada a sus realidad y necesidades. Esto va a ser una perdida para un mundo que necesita más invención y más diversidad en su innovación. En esta situación, se va a ampliar aún más la información obsoleta dentro de los modelos de datos, ampliando la exclusión para aquellos grupos históricamente marginalizados de mujeres y niñas. [^1]  


Es importante hacer notar que las mujeres y niñas pueden y son una forma de darle voz a todos aquellos grupos tradicionalmente invisibles, aquellos tradicionalmente dejados atrás. La información de datos feminista es inseparable de la “interseccionalidad”, “la naturaleza interconectada de categorías sociales como la clase, raza y genero mientras se aplican a cierto individuo o grupo, considerado como una superposición y sistemas interdependientes de discriminación y desventaja”. Nuestra discusión es igualmente poderosa (con ese propósito) y aplicable a otras formas de discriminación, más notablemente discriminación racial. [^2]  

Estamos en un punto de inflección.  

# El Paisaje

## Parcialidad de género en los algoritmos y decisiones automáticas  


### Parcialidad inherente al contratar

Para optimizar los recursos humanos Amazon creo un algoritmo derivado de 10 años de currículos enviados a Amazon. Usó información comparada con el equipo de alto rendimiento, principalmente de hombres, de ingeniería. Este algoritmo se le enseño como reconocer patrones de palabras, en lugar de habilidades importantes en los currículos, y al ver que históricamente los hombres eran contratados y promovidos, el algoritmo se enseño a si mismo a penalizar cualquier resumen que incluyera la palabra “mujeres”, por ejemplo “capitana del equipo de ajedrez de mujeres” en el texto y degradar resúmenes de mujeres que hubieran ido a “universidades de mujeres” [^3]. Esto se debe a que la información con la que se entrenó el algoritmo contiene parcialidad o discriminación histórica, crea un bucle donde el aprendizaje automático absorbe esta parcialidad y lo replica, incorporándolo a decisiones futuras, y hace que esta parcialidad implícita se vuelva una realidad explicita. [^4]  
 
 A pesar de múltiples intentos fracasados de arreglar el algoritmo y quitarle la parcialidad, la cual se asumió que iba a ser un simple arreglo técnico, Amazon eventualmente desecho el algoritmo en su totalidad ya que la parcialidad estaba demasiado arraigada en los procesos de reclutamiento anteriores. Esta parcialidad estaba profundamente implícita en la información en la cual el algoritmo se entrenó y el aprendizaje automático del sistema ADM no pudo “desaprenderlo” [^5]. En el 2017, Amazon abandonó el proyecto y compartió su información y experiencia con el público por Reuters.   
 
 La Inteligencia Artificial permea los procesos modernos de reclutamiento desde web crawlers para identificar y atraer a candidatos favoritos a través de sistemas de rastreo de aplicantes, curadores de currículos, evaluaciones clásicas, entrevistas automatizada, análisis de entrevistas y sistemas de calificación de candidatos. Un reporte reciente estima que el 99% de las compañías del Fortune 500 utilizan sistemas de rastreo de aplicantes de algún tipo en sus procesos de reclutamiento [^6]. La AI está esperada que reemplace el 16% de los trabajos de recursos en los próximos diez años [^7], lo cual significa que la dependencia multinacional de este tipo de software y procesos y accesos a la información (o la falta de) va a estar en crecimiento. 
 
 
 ### Parcialidad al seleccionar y los estereotipos
 
 Un estudio del 2019 de la funcionalidad de anuncios de Facebook encontró que los anuncios para los trabajos en la industria maderera eran mostrados desproporcionalmente a usuarios blancos masculinos, mientras que anuncios para posiciones de cajeros en supermercados eran enseñados a usuarios femeninos [^8]. Permitiendo que los anunciantes de trabajos solo llegaran a hombres, sin estar necesariamente de acuerdo o consientes de esto, se mostraron anuncios de una manera que se alineaba con los estereotipos de genero [^9]. Cuando un algoritmo “aprende” un patrón de que más hombres que mujeres están interesados en la industria maderera (incluso si no sabe su género y aprende esto correlacionando otra información sobre sus gustos y hábitos), entonces el sistema decide no mostrarles esos trabajos a otras mujeres, simplemente porque son mujeres [^10]. Esto exacerba los estereotipos existentes y las barreras en las sociedades que han excluido a las mujeres mucho antes de los sistemas automatizados.   
 
 Después de que Facebook fuera demandado por estas prácticas, como parte del arreglo a inicios del 2019, Facebook arregló cinco demandas y accedió detener a los anunciantes en ciertas categorías claves de poder llegar a grupos exclusivos de cierto tipo de personas, género o raza [^11].  
 
 Sin embargo, los algoritmos funcionan diferente en la teoría y en la práctica. Aun cuando Facebook quitó la funcionalidad de poder acceder a grupos específicos de personas, su complejo algoritmo en el 2021 depende de otras múltiples características que al final sirven como puente para llegar al género, raza y edad. Al final los resultados siempre terminan siendo parciales [^12].   
 
 Muchas veces los anuncios parecen deliberadamente tener estereotipos. Una encuesta de ProPublica encontró que la policía del estado de Pennsylvania en los Estados Unidos, por ejemplo, puso un anunció exclusivamente a hombres con el texto que decía: “La policía del estado de Pennsylvania inician con un salario de $59,567 al año. Aplique ahora.” [^13]. Los objetivos por genero es solo una manera en la cual las plataformas permiten a los anunciantes enfocarse en ciertos usuarios y excluir a otros, y por inadvertencia, denegarles la oportunidad de aplicar a trabajos con más paga y de más estatus.   
 
 ### Estereotipos implícitos y parcialidad inconsciente transducida a misoginia explicita  
 
 Investigadores de una compañía pionera de tecnología en Estados Unidos dicen tener un 97% de eficacia en un sistema de reconocimiento facial que diseñaron, sin embargo, la información con la que se construyó el sistema era mas de 77% hombres y mas de un 83% blancos [^14]. Investigadores del MIT y de Stanford en los Estados Unidos probaron tres sistemas de reconocimiento facial por IBM, Microsoft, Megvii, y encontrar que estos sistemas eran buenos para reconocer hombres blancos, pero no mujeres, específicamente si tenían un tono de piel mas oscuro [^15].   
 
 En el 2017 un grupo de investigadores encontró que dos prominentes colecciones de imágenes dedicadas para la investigación, incluyendo una soportada por Microsoft y Facebook, desplegaban una parcialidad de genero predecible en su representación de actividades como cocinar y deportes. Por ejemplo, imágenes de compras y lavado de platos estaban vinculadas con mujeres, mientras que imágenes de entramiento y disparos están vinculadas a hombres. De la misma manera, objetos de cocina como cucharas y tenedores están asociados fuertemente con mujeres, mientras que equipo de actividades al aire libre como tablas para hacer snowboarding y raquetas de tenis están asociadas a hombres. Otro lado de está información es la escasez geográfica de la información, en un estudio se muestra que los algoritmos muestran a la novia en una boda típica estadounidense de blanco como “novia”, “vestido”, “mujer”, mientras que una foto de una novia del norte de la india está clasificada como “arte”, “disfraz”. La parcialidad de la información usada para entrenar estos algoritmos sobre representa una población, mientras que minimiza otra [^16].    
 
 En el 2019 ImageNet quitó 600,000 imágenes de su sistema (entrenado con WordNet, la base de datos de palabras en Ingles usadas en lingüística computacional y procesamiento del lenguaje natural) después de que un proyecto de arte mostrado ImageNet Roulette ilustrara una parcialidad sistemática en su base de datos. En el 2020, el MIT quitó permanentemente 80 millones de imágenes pequeñas de bases de datos cuando descubrieron que estaban etiquetadas con contenido racista y misógino, produciendo resultados que inadvertidamente reafirmaban estereotipos y sesgos dañinos [^17].   
 
 El aprendizaje automático entrenado en estas bases de datos no solo replico estas parcialidades, las amplifico. Si un set de fotos está generalmente asociado con mujeres cocinando, este tipo de software entrenado estudiando esas fotos, crea asociaciones aun más fuertes. En las pruebas de los investigadores, personas que estaban en cocinas estaban más propensas a ser etiquetadas como “mujeres”: en una foto de un hombre en una cocina se le etiqueta como “mujer” [^18]. De la misma manera, investigadores de la Universidad de Washington y Maryland encontraron que algunos términos de búsqueda como “Chief Executive Officer” (CEO), Google presentaba porcentaje peores que las estadísticas actuales de la vida real. El estudio concluyó que 11% de las personas mostradas en la búsqueda de imágenes de CEO eran mujeres, mientras que la información muestra que, al momento de realizar la investigación, 27% de las mujeres en Estados Unidos eran CEOs. [^19]   
 
 Igualmente, las incrustaciones de palabras entrenadas en los artículos de Google News demuestran estereotipos de genero propagados diariamente. Por ejemplo, ocupaciones de “ella” incluyen ama de casa, bibliotecaria, niñera mientras que ocupaciones para “ellos” incluyen profesor, filósofo y financiero [^20]. Investigadores de la Universidad de Boston y Microsoft muestran que software entrenado con texto coleccionado de Google News reproduce estereotipos de géneros bien documentados en humanos. Cuando le preguntaron al software que completara la afirmación “Hombre es a la computadora programador, como mujer es a X” respondió con “ama de casa. [^21]  
 
 Similarmente, cuando Google Traductor convierte artículos de noticias escritos en español a Ingles, frases referentes a mujeres profesionales tales como profesora, muchas veces se traducían a “el dijo” o “el escribió” [^22]. En el lenguaje Turco donde no hay “el” o “ella”, Google Traductor creó emparejamientos relacionados al género cuando el lenguaje no tienen ninguno, y estos resultados fueron sumamente chocantes, “ella es una cocinera”, “el es un ingeniero”, “el es un doctor”, “ella es una enfermera”, “el trabaja duro, “ella es una perezosa” [^23]. Otros ejemplos de parcialidad de genero inconsciente incluyen la prevalencia de maquinas feminizadas como Alexa, Google Home y Siri, todas tienen una voz femenina por defecto (Google Home y Siri pueden cambiar a vos masculina). Sin embargo, el software de reconocimiento de voz está entrenado en grabaciones de voces masculinas. Por lo tanto, la versión de Google es 70% más capaz de entender a hombres y por lo tanto el hombre comanda a la mujer “asistente”. [^24]   
 
 Una campaña desarrollada por mujeres UN en el 2013, reveló la discriminación esparcida contra las mujeres y el sexismo a través del uso de la función de autocompletado de Google. La campaña contaba con fotos de cerca de perfiles de mujeres con los resultados del algoritmo de autocompletado con búsquedas como “las mujeres no deberían…” y “las mujeres tienen que…”. Ejemplos de los resultados que se autocompletaron fueron “las mujeres…no deberían tener derechos” y “las mujeres…necesitan ser disciplinadas”. [^25]   
 
 Nosotros esperamos que las cosas cambiaron del 2013 al 2019, pero en el 2019 en el trabajo The Woman Worked as a Babysitter: On Biases in Language Generation, los investigadores produjeron los siguientes ejemplos de continuaciones de texto generados desde el OpenAI GPT-2: [^26]  
 
 
 ### Nosotros formamos nuestras herramientas de la misma manera que nuestras herramientas nos forman[^27]

Hemos reaccionado lento a las montañas de evidencia de parcialidad género, sexismo, racismo, transfobia y homofobia que son penetrantes en línea. “Deep fakes”, personas reales, perfiles reales, cercanos a contenido fotorrealista y eventos completamente falsos continúan existiendo en línea [^28]. DeepNude, una aplicación que desvestía a mujeres con un solo click fue lanzada (y seguidamente quitada debido a quejas del publico una vez que recibió atención) [^29]. Igualmente, preocupante, Twitter ha patrocinado tweets promoviendo software de espionaje diseñado para monitorear novias y esposas con la frase “que te esta escondiendo? ¡Descúbrelo con mSpy!” [^30]. En el 2021, algoritmos de generación de imágenes alimentados con fotos de un hombre con su cuerpo removido debajo de su cuello, lo autocompletaría un 43% con un traje, si se le alimentaba una foto de una mujer, incluso alguna famosa como la representante Alexandria Ocasio-Cortez, la autocompletaría con un bikini el 53% del tiempo [^31]. También en el 2021 un DeepFake bot, gratis, fácil de usar, en la aplicación de Telegram, reemplazaba ropa con desnudez, su dueño reporta que más de 700,000 imágenes han sido creadas. Muchas son de menores de edad, todas mujeres. [^32]   

“Nosotros formamos nuestras herramientas de la misma manera que nuestras herramientas nos forman” [^33]. Este es nuestro reto inmediato. Necesitamos establecer nuevas herramientas y normas, para poder tener un cambio que perdure en nuestros sistemas culturas e institucionales en nuestro siglo y después. Esto es problema de todas personas en todas las esquinas del mundo. Es crucial que nos enfoquemos en la igualdad de genero y en valores que son esenciales para la democracia, para hombres y mujeres, ahora. 

Nuestra propuesta es capturar el momento antes de que el aprendizaje automático domine los sistemas globales. Buscamos movilizar un amplio grupo de multi disciplinadas feministas las cuales son investigadores de AI, Ingenieras en Computación, científicas de datos, especialistas de aprendizaje automático junto con Economistas, Investigadoras Sociales, especialistas de genero, antropólogas, politólogas, trabajadoras sociales, filosofas, psicólogas y activistas para poder explorar nuevas definiciones de problemas, modelos de información y innovaciones feministas en AI a través de la investigación aplicada. Queremos crear investigación que vaya más allá de describir la parcialidad o mitigar la parcialidad que está tan claramente incrustada en los sistemas análogos como digitales con los que vivimos, para corregir esta parcialidad usando el poder total de la tecnología para poder avanzar los valores de la igualdad que hemos acogido, y el potencial de la tecnología de poder forjar nuevos sistemas si nos enfocamos en los actos positivos de la creación.     

Nosotros soñamos de iniciar investigaciones colaborativas que pongan en movimiento el uso de potencial históricamente inutilizado, imaginación y las habilidades de las mujeres y niñas del sur global, norte, este y oeste y junto a ellas emplear efectivamente nuevas maneras de recolectar tecnología que *corrige y supera* las parcialidades de la vida real y las barreras que evitan que las mujeres participen activamente y derechos para el presente y el future que inventemos. [^34] 
### Bibliografia
Ali, Muhammad, Piotr Sapiezynski, Miranda Bogen, Aleksandra Korolova, Alan Mislove, and Aaron Rieke. ‘Discrimination through Optimization: How Facebook’s Ad Delivery Can Lead to Skewed Outcomes’. Proceedings of the ACM on Human-Computer Interaction 3, no. CSCW (7 November 2019): 1–30. https://doi.org/10.1145/3359301.  
AZEVÊDO, DG ROBERTO. More work needed to tackle barriers women entrepreneurs face in trade says DG Azevêdo, 3 July 2019. https://www.wto.org/english/news_e/spra_e/spra275_e.htm.  
Bolukbasi, Tolga, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai. ‘Man Is to Computer Programmer as Woman Is to Homemaker? Debiasing Word Embeddings’. ArXiv:1607.06520 [Cs, Stat], 21 July 2016. http://arxiv.org/abs/1607.06520.  
Callahan, Molly. ‘Facebook’s Ad Delivery System Still Discriminates by Race, Gender, Age’. Northeastern University, 18 December 2019. https://news.northeastern.edu/2019/12/18/facebooks-ad-delivery-system-still-discriminates-by-race-gender-age-y/.  
Chivers, Tom. ‘What Do We Do about Deepfake Video?’, 23 June 2019, sec. Technology. http://www.theguardian.com/technology/2019/jun/23/what-do-we-do-about-deepfake-video-ai-facebook.  
Cole, Samantha. ‘Deepnude: The Horrifying App Undressing Women’. Vice. Accessed 16 March 2021. https://www.vice.com/en/article/kzm59x/deepnude-app-creates-fake-nudes-of-any-woman.  
Cox, Joseph. ‘Twitter Pushed Adverts for Spyware to Monitor Girlfriends’. VICE. Accessed 16 March 2021. https://www.vice.com/en/article/3k3wx5/twitter-pushed-adverts-for-spyware-to-track-girlfriends.  
Forbes Coaches Council. ‘Council Post: 10 Ways Artificial Intelligence Will Change Recruitment Practices’. Forbes, 10 August 2018, sec. Leadership. https://www.forbes.com/sites/forbescoachescouncil/2018/08/10/10-ways-artificial-intelligence-will-change-recruitment-practices/.  
‘Gender Working Group | Global Research Council’. Accessed 16 March 2021. https://www.globalresearchcouncil.org/about/gender-working-group/.  
Global Research Council. Supporting Women in Research. Policies, Programs and Initiatives Undertaken by Public Research Funding Agencies. Accessed 16 March 2021. https://www.globalresearchcouncil.org/about/gender-working-group/.  
Gonzalez, Anabel. ‘What’s Challenging Women as They Seek to Trade and Compete in the Global Economy’. World Bank Blogs (blog), 17 July 2017. https://blogs.worldbank.org/trade/what-s-challenging-women-they-seek-trade-and-compete-global-economy.  
‘GRC_GWG_Case_studies_final.Pdf’. Accessed 22 March 2021. https://www.globalresearchcouncil.org/fileadmin/documents/GWG/GRC_GWG_Case_studies_final.pdf.  
Hao, Karen. ‘An AI Saw a Cropped Photo of AOC. It Autocompleted Her Wearing a Bikini. | MIT Technology Review’. MIT Technology Review, 29 January 2021. https://www.technologyreview.com/2021/01/29/1017065/ai-image-generation-is-racist-sexist/.  
Hardesty, Larry. ‘Study Finds Gender and Skin-Type Bias in Commercial Artificial-Intelligence Systems’. MIT News | Massachusetts Institute of Technology, 11 February 2018. https://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212.  
Kay, Matthew, Cynthia Matuszek, and Sean A. Munson. ‘Unequal Representation and Gender Stereotypes in Image Search Results for Occupations’. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, 3819–28. CHI ’15. New York, NY, USA: Association for Computing Machinery, 2015. https://doi.org/10.1145/2702123.2702520.  
Liu, Katherine A., and Natalie A. Dipietro Mager. ‘Women’s Involvement in Clinical Trials: Historical Perspective and Future Implications’. Pharmacy Practice 14, no. 1 (2016). https://doi.org/10.18549/PharmPract.2016.01.708.  
Mahdawi, Arwa. ‘An App Using AI to “undress” Women Offers a Terrifying Glimpse into the Future’. the Guardian, 29 June 2019. http://www.theguardian.com/commentisfree/2019/jun/29/deepnude-app-week-in-patriarchy-women.  
McLuhan, Marshall. Understanding Media: The Extensions of Man. Routledge, 1994.  
Merrill, Jeremy B. ‘Facebook’s Algorithm Makes Some Ads Discriminatory—All on Its Own’. Quartz. Accessed 16 March 2021. https://qz.com/1588428/new-research-suggests-facebooks-algorithm-may-be-discriminatory/.  
Olson, Parmy. ‘The Algorithm That Helped Google Translate Become Sexist’. Forbes, sec. Tech. Accessed 16 March 2021. https://www.forbes.com/sites/parmyolson/2018/02/15/the-algorithm-that-helped-google-translate-become-sexist/.  
Perez, Caroline Criado. Invisible Women: Data Bias in a World Designed for Men. First Printing edition. New York: Harry N. Abrams, 2019.  
———. ‘The Deadly Truth about a World Built for Men – from Stab Vests to Car Crashes’. the Guardian, 23 February 2019. http://www.theguardian.com/lifeandstyle/2019/feb/23/truth-world-built-for-men-car-crashes.  
Qu, Linda. ‘Report: 99% of Fortune 500 Companies Use Applicant Tracking Systems’. Jobscan (blog), 7 November 2019. https://www.jobscan.co/blog/99-percent-fortune-500-ats/.  
Scheiber, Noam. ‘Facebook Accused of Allowing Bias Against Women in Job Ads - The New York Times’. New York Times, 18 September 2018. https://www.nytimes.com/2018/09/18/business/economy/facebook-job-ads.html.  
Scheiber, Noam, and Mike Isaac. ‘Facebook Halts Ad Targeting Cited in Bias Complaints - The New York Times’. New York Times, 19 March 2019. https://www.nytimes.com/2019/03/19/technology/facebook-discrimination-ads.html.  
Schiebinger, Londa, and ineke klinge. ‘Gendered Innovations: How Gender Analysis Contributes to Research’. “Innovation through Gender. EUROPEAN COMMISSION, 2013. https://ec.europa.eu/research/science-society/document_library/pdf_06/gendered_innovations.pdf.  
Sheng, Emily, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. ‘The Woman Worked as a Babysitter: On Biases in Language Generation’. ArXiv:1909.01326 [Cs], 23 October 2019. http://arxiv.org/abs/1909.01326.  
Song, Victoria. ‘MIT Takes Down Popular AI Dataset Due to Racist, Misogynistic Content’. Gizmodo, 7 February 2020. https://gizmodo.com/mit-takes-down-popular-ai-dataset-due-to-racist-misogy-1844244206.  
Tobin, Ariana, and Jeremy B. Merrill. ‘Facebook Is Letting Job Advertisers Target Only Men’. ProPublica, 18 September 2018. https://www.propublica.org/article/facebook-is-letting-job-advertisers-target-only-men?token=xbvF5KLcDIV6vr6B2AF9D0LlUK_IwLni.  
UN Women. ‘UN Women Ad Series Reveals Widespread Sexism’, 21 October 2013. https://www.unwomen.org/en/news/stories/2013/10/women-should-ads.  
UNESCO. ‘I’d Blush If I Could: Closing Gender Divides in Digital Skills through Education’. UNESCO, 18 March 2019. https://en.unesco.org/Id-blush-if-I-could.  
‘What Is Automated Individual Decision-Making and Profiling?’ ICO, 4 January 2021. https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/automated-decision-making-and-profiling/what-is-automated-individual-decision-making-and-profiling/.  
Zhao, Jieyu, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. ‘Men Also Like Shopping: Reducing Gender Bias Amplification Using Corpus-Level Constraints’. ArXiv:1707.09457 [Cs, Stat], 28 July 2017. http://arxiv.org/abs/1707.09457.  



[^1]:
[^2]:
[^3]:
[^4]:
[^5]:
[^6]:
[^7]:
[^8]:
[^9]:
[^10]:
[^11]:
[^12]:
[^13]:
[^14]:
[^15]:
[^16]:
[^17]:
[^18]:
[^19]:
[^20]:
[^21]:
[^22]:
[^23]:
[^24]:
[^25]:
[^26]:
[^27]:
[^28]:
[^29]:
[^30]:
[^31]:
[^32]:
[^33]:
[^34]:
